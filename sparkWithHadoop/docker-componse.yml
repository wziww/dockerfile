# http://github.com/yeasy/docker-compose-files
# This compose file will start spark master node and the worker node.
# All nodes will become a cluster automatically.
# You can run: docker-compose scale worker=2
# After startup, try submit a pi calculation application.
#  /urs/local/spark/bin/spark-submit --master spark://master:7077 --class org.apache.spark.examples.SparkPi /usr/local/spark/lib/spark-examples-1.4.0-hadoop2.6.0.jar 1000

master:
  image: ms/spark:latest
  hostname: master
  ports:
  - "4040:4040"
  - "8042:8042"
  - "7077:7077"
  - "8088:8088"
  - "8081:8080"
  - "9000:9000"
  - "50070:50070"
  restart: always
  command: bash -c "/usr/sbin/sshd -D & && /usr/local/hadoop-spark/spark/sbin/start-master.sh && ping localhost > /dev/null"

worker:
  image: ms/spark:latest
  links:
  - master:master
  expose:
  - "8081"
  restart: always
  command: bash -c "/usr/sbin/sshd -D & && /usr/local/hadoop-spark/spark/sbin/start-slave.sh spark://master:7077 && ping localhost >/dev/null"